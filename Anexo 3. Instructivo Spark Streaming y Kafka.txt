Universidad Nacional Abierta y a Distancia
Vicerrectoría Académica y de Investigación
Unidad gestora: Escuela
Programa: Ingeniería de Sistemas
Curso: Big Data
Código: 202016911
Análisis de Datos en Tiempo Real con Spark Streaming y Kafka
Se debe hacer uso de la máquina virtual configurada con hadoop y spark
trabajada en las actividades anteriores.
Usuario: vboxuser
Password: bigdata
Ejecutamos Putty para conectarnos por SSH a la máquina virtual utilizando la IP
local
Y nos logueamos nuevamente
Usuario: vboxuser
Password: bigdata
Instalamos mediante PIP la librería de Python Kafka
pip install kafka-python
Descargue, descomprima y mueva de carpeta Apache Kafka
wget https://downloads.apache.org/kafka/3.6.2/kafka_2.13-3.6.2.tgz
tar -xzf kafka_2.13-3.6.2.tgz
sudo mv kafka_2.13-3.6.2 /opt/Kafka
si solicita password: bigdata
Iniciamos el servidor ZooKeeper:
sudo /opt/Kafka/bin/zookeeper-server-start.sh
/opt/Kafka/config/zookeeper.properties &
Después de un momento y terminada la ejecución del comando anterior se debe
dar Enter para que aparezca nuevamente el prompt del sistema
Iniciamos el servidor Kafka:
sudo /opt/Kafka/bin/kafka-server-start.sh /opt/Kafka/config/server.properties &
Después de un momento y terminada la ejecución del comando anterior se debe
dar Enter para que aparezca nuevamente el prompt del sistema
Creamos un tema (topic) de Kafka, el tema se llamará sensor_data
/opt/Kafka/bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --
replication-factor 1 --partitions 1 --topic sensor_data
Implementación del productor(producer) de Kafka
Creamos un archivo llamado kafka_producer.py
nano kafka_producer.py
con el siguiente contenido:
import time
import json
import random
from kafka import KafkaProducer
def generate_sensor_data():
return {
"sensor_id": random.randint(1, 10),
"temperature": round(random.uniform(20, 30), 2),
"humidity": round(random.uniform(30, 70), 2),
"timestamp": int(time.time())
}
producer = KafkaProducer(bootstrap_servers=['localhost:9092'],
value_serializer=lambda x: json.dumps(x).encode('utf-8'))
while True:
sensor_data = generate_sensor_data()
producer.send('sensor_data', value=sensor_data)
print(f"Sent: {sensor_data}")
time.sleep(1)
dar Crtl+O enter y luego Crtl+X para salir
Este script genera datos simulados de sensores y los envía al tema (topic) de
Kafka que creamos anteriormente (sensor_data).
Implementación del consumidor con Spark Streaming
Ahora, crearemos un consumidor(consumer) utilizando Spark Streaming para
procesar los datos en tiempo real. Crea un archivo llamado
spark_streaming_consumer.py
nano spark_streaming_consumer.py
con el siguiente contenido:
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col, window
from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, TimestampType
import logging
# Configura el nivel de log a WARN para reducir los mensajes INFO
spark = SparkSession.builder \
.appName("KafkaSparkStreaming") \
.getOrCreate()
spark.sparkContext.setLogLevel("WARN")
# Definir el esquema de los datos de entrada
schema = StructType([
StructField("sensor_id", IntegerType()),
StructField("temperature", FloatType()),
StructField("humidity", FloatType()),
StructField("timestamp", TimestampType())
])
# Crear una sesión de Spark
spark = SparkSession.builder \
.appName("SensorDataAnalysis") \
.getOrCreate()
# Configurar el lector de streaming para leer desde Kafka
df = spark \
.readStream \
.format("kafka") \
.option("kafka.bootstrap.servers", "localhost:9092") \
.option("subscribe", "sensor_data") \
.load()
# Parsear los datos JSON
parsed_df = df.select(from_json(col("value").cast("string"), schema).alias("data")).select("data.*")
# Calcular estadísticas por ventana de tiempo
windowed_stats = parsed_df \
.groupBy(window(col("timestamp"), "1 minute"), "sensor_id") \
.agg({"temperature": "avg", "humidity": "avg"})
# Escribir los resultados en la consola
query = windowed_stats \
.writeStream \
.outputMode("complete") \
.format("console") \
.start()
query.awaitTermination()
dar Crtl+O enter y luego Crtl+X para salir
Este script utiliza Spark Streaming para leer datos del tema(topic) de Kafka,
procesa los datos en ventanas de tiempo de 1 minuto y calcula la temperatura y
humedad promedio para cada sensor.
Ejecución y análisis
En una terminal, ejecutamos el productor(producer) de Kafka:
python3 kafka_producer.py
En otra terminal, ejecutamos el consumidor de Spark Streaming:
Ejecutamos otra terminal de Putty para conectarnos por SSH a la máquina virtual
utilizando la IP local sin cerrar la otra terminal de Putty
Y nos logueamos nuevamente
Usuario: vboxuser
Password: bigdata
spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3
spark_streaming_consumer.py
Observamos la salida del productor y del consumidor de Spark Streaming y
analizamos los resultados.
NOTA: es importante primero ejecutar el script de productor y luego el del
consumidor
Podemos ver información sobre los Jobs, Stages, entre otros, realizados por
Spark, para esto ingresamos a la consola web
http://your-server-ip:4040
Para este caso http://192.168.1.7:4040


Para finalizar la ejecución de los scripts en Putty damos CTRL+C