# Spark E-commerce Analytics

## Descripci√≥n del Proyecto

Este proyecto implementa una soluci√≥n completa de an√°lisis de datos de transacciones de e-commerce utilizando Apache Spark para procesamiento batch y streaming en tiempo real con Apache Kafka.

### Caracter√≠sticas Principales

- **Procesamiento Batch**: An√°lisis exploratorio de datos (EDA) de 100,000 transacciones de ventas
- **Procesamiento en Tiempo Real**: Streaming de transacciones simuladas usando Kafka y Spark Streaming
- **An√°lisis Completo**: Estad√≠sticas por categor√≠a, m√©todo de pago, productos m√°s vendidos, an√°lisis temporal
- **Visualizaci√≥n**: Resultados en consola y archivos Parquet para an√°lisis posterior

## Dataset Utilizado

El proyecto utiliza un dataset sint√©tico de transacciones de e-commerce que incluye:

- **100,000 transacciones** generadas aleatoriamente
- **5 categor√≠as**: Electr√≥nica, Ropa, Hogar, Deportes, Libros
- **3 m√©todos de pago**: Tarjeta, PayPal, Transferencia
- **3 estados**: Completada, Pendiente, Cancelada
- **Per√≠odo**: √öltimos 30 d√≠as con timestamps realistas

### Campos del Dataset

| Campo | Tipo | Descripci√≥n |
|-------|------|-------------|
| transaction_id | Integer | ID √∫nico de la transacci√≥n |
| timestamp | String | Fecha y hora de la transacci√≥n |
| customer_id | Integer | ID del cliente |
| product_id | Integer | ID del producto |
| category | String | Categor√≠a del producto |
| quantity | Integer | Cantidad vendida |
| unit_price | Double | Precio unitario |
| total | Double | Total de la transacci√≥n |
| payment_method | String | M√©todo de pago utilizado |
| status | String | Estado de la transacci√≥n |

## Requisitos Previos

### En la M√°quina Virtual (Ubuntu con Hadoop/Spark)

- Apache Spark 3.5.3
- Apache Kafka 3.6.2
- Python 3.x
- Java 8 o superior

### Dependencias Python

```bash
pip install -r requirements.txt
```

## Instrucciones de Instalaci√≥n

### 1. Configuraci√≥n de Kafka (en la VM)

```bash
# Instalar kafka-python
pip install kafka-python

# Descargar y configurar Kafka
wget https://downloads.apache.org/kafka/3.6.2/kafka_2.13-3.6.2.tgz
tar -xzf kafka_2.13-3.6.2.tgz
sudo mv kafka_2.13-3.6.2 /opt/Kafka

# Iniciar ZooKeeper
sudo /opt/Kafka/bin/zookeeper-server-start.sh /opt/Kafka/config/zookeeper.properties &

# Iniciar Kafka
sudo /opt/Kafka/bin/kafka-server-start.sh /opt/Kafka/config/server.properties &

# Crear topic para transacciones
/opt/Kafka/bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic transacciones_ventas
```

## Comandos de Ejecuci√≥n

### 1. Generar Dataset (Ejecutar localmente)

```bash
python3 generar_dataset_ventas.py
```

### 2. Procesamiento Batch (En la VM)

```bash
spark-submit procesamiento_batch.py
```

### 3. Streaming en Tiempo Real (En la VM)

#### Terminal 1 - Productor Kafka:
```bash
python3 kafka_productor_ventas.py
```

#### Terminal 2 - Consumidor Spark Streaming:
```bash
spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3 spark_streaming_consumidor.py
```

## Estructura del Proyecto

```
spark-ecommerce-analytics/
‚îú‚îÄ‚îÄ datos/
‚îÇ   ‚îî‚îÄ‚îÄ transacciones_ventas.csv          # Dataset generado
‚îú‚îÄ‚îÄ batch/
‚îÇ   ‚îî‚îÄ‚îÄ procesamiento_batch.py            # Script de procesamiento batch
‚îú‚îÄ‚îÄ streaming/
‚îÇ   ‚îú‚îÄ‚îÄ kafka_productor_ventas.py         # Productor de Kafka
‚îÇ   ‚îî‚îÄ‚îÄ spark_streaming_consumidor.py    # Consumidor Spark Streaming
‚îú‚îÄ‚îÄ resultados/                            # Resultados del procesamiento batch
‚îÇ   ‚îú‚îÄ‚îÄ transacciones_limpias.parquet
‚îÇ   ‚îú‚îÄ‚îÄ transacciones_con_fecha.parquet
‚îÇ   ‚îî‚îÄ‚îÄ ventas_por_categoria.parquet
‚îú‚îÄ‚îÄ README.md                             # Este archivo
‚îú‚îÄ‚îÄ requirements.txt                       # Dependencias Python
‚îî‚îÄ‚îÄ INSTRUCCIONES_EJECUCION.md           # Gu√≠a detallada de ejecuci√≥n
```

## An√°lisis Realizados

### Procesamiento Batch

1. **Limpieza de Datos**
   - Eliminaci√≥n de valores nulos
   - Eliminaci√≥n de duplicados
   - Validaci√≥n de datos

2. **An√°lisis Exploratorio**
   - Estad√≠sticas descriptivas
   - Ventas por categor√≠a
   - Ventas por m√©todo de pago
   - Top 10 productos m√°s vendidos
   - An√°lisis temporal (ventas por d√≠a/hora)
   - Distribuci√≥n de estados de transacci√≥n

### Procesamiento en Tiempo Real

1. **M√©tricas por Ventana de Tiempo (1 minuto)**
   - Total de ventas
   - N√∫mero de transacciones
   - Promedio de venta

2. **An√°lisis por Categor√≠a**
   - Ventas por categor√≠a en tiempo real
   - Ranking de categor√≠as m√°s vendidas

3. **An√°lisis de M√©todos de Pago**
   - M√©todos m√°s utilizados
   - Volumen de transacciones por m√©todo

4. **Estados de Transacci√≥n**
   - Distribuci√≥n de estados en tiempo real
   - Monitoreo de transacciones completadas vs pendientes

## Monitoreo y Visualizaci√≥n

### Interfaz Web de Spark
Acceder a: `http://192.168.0.x:4040` para ver:
- Jobs ejecut√°ndose
- Stages del procesamiento
- M√©tricas de rendimiento
- Historial de aplicaciones

### Salida en Consola
Los resultados se muestran en tiempo real en la consola con:
- Ventanas de tiempo con m√©tricas agregadas
- Rankings de categor√≠as y productos
- Estad√≠sticas de m√©todos de pago

## Resultados Esperados

### Procesamiento Batch
- Dataset limpio de ~100,000 transacciones
- Estad√≠sticas por categor√≠a (Electr√≥nica lidera en ventas)
- Distribuci√≥n temporal de ventas
- Top productos m√°s vendidos

### Streaming en Tiempo Real
- Procesamiento continuo de transacciones
- Actualizaci√≥n de m√©tricas cada minuto
- Monitoreo de patrones de venta en tiempo real
- Detecci√≥n de tendencias por categor√≠a

## Soluci√≥n de Problemas

### Error de Conexi√≥n a Kafka
```bash
# Verificar que Kafka est√© ejecut√°ndose
sudo /opt/Kafka/bin/kafka-topics.sh --list --bootstrap-server localhost:9092
```

### Error de Memoria en Spark
```bash
# Ajustar memoria en spark-submit
spark-submit --driver-memory 2g --executor-memory 2g procesamiento_batch.py
```

### Puerto 4040 Ocupado
```bash
# Cambiar puerto de la interfaz web
spark-submit --conf spark.ui.port=4041 procesamiento_batch.py
```

## Autor

**appsgalvis** - Estudiante de Ingenier√≠a de Sistemas UNAD
Curso: Big Data (202016911)
Tarea 3: Procesamiento de Datos con Apache Spark

## Repositorio GitHub

üîó **URL del Repositorio**: [https://github.com/appsgalvis/spark-ecommerce-analytics](https://github.com/appsgalvis/spark-ecommerce-analytics)

## Licencia

Este proyecto es parte de una actividad acad√©mica de la Universidad Nacional Abierta y a Distancia (UNAD).