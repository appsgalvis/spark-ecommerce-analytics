# Spark E-commerce Analytics

## Descripci√≥n del Proyecto

Este proyecto implementa una soluci√≥n completa de an√°lisis de datos de transacciones de e-commerce utilizando Apache Spark para procesamiento batch y streaming en tiempo real con Apache Kafka.

### Caracter√≠sticas Principales

- **Procesamiento Batch**: An√°lisis exploratorio de datos (EDA) de 100,000 transacciones de ventas
- **Procesamiento en Tiempo Real**: Streaming de transacciones simuladas usando Kafka y Spark Streaming
- **An√°lisis Completo**: Estad√≠sticas por categor√≠a, m√©todo de pago, productos m√°s vendidos, an√°lisis temporal
- **Visualizaci√≥n**: Resultados en consola y archivos Parquet para an√°lisis posterior

## Dataset Utilizado

El proyecto utiliza un dataset sint√©tico de transacciones de e-commerce que incluye:

- **100,000 transacciones** generadas aleatoriamente
- **5 categor√≠as**: Electr√≥nica, Ropa, Hogar, Deportes, Libros
- **3 m√©todos de pago**: Tarjeta, PayPal, Transferencia
- **3 estados**: Completada, Pendiente, Cancelada
- **Per√≠odo**: √öltimos 30 d√≠as con timestamps realistas

### Campos del Dataset

| Campo | Tipo | Descripci√≥n |
|-------|------|-------------|
| transaction_id | Integer | ID √∫nico de la transacci√≥n |
| timestamp | String | Fecha y hora de la transacci√≥n |
| customer_id | Integer | ID del cliente |
| product_id | Integer | ID del producto |
| category | String | Categor√≠a del producto |
| quantity | Integer | Cantidad vendida |
| unit_price | Double | Precio unitario |
| total | Double | Total de la transacci√≥n |
| payment_method | String | M√©todo de pago utilizado |
| status | String | Estado de la transacci√≥n |

## Requisitos Previos

### En la M√°quina Virtual (Ubuntu con Hadoop/Spark)

**Nota Importante**: Este proyecto asume que ya tienes un servidor Hadoop preconfigurado y funcionando en la m√°quina virtual con los siguientes componentes instalados:

- Apache Spark 3.5.3 (preinstalado y configurado)
- Python 3.x (preinstalado)
- Java 8 o superior (preinstalado)
- Apache Kafka 3.6.2 (se instalar√° durante el proceso)

### Dependencias Python

```bash
pip install -r requirements.txt
```

## Instrucciones de Instalaci√≥n

**Nota**: El servidor Hadoop y Spark ya deben estar preconfigurados y funcionando correctamente. Estas instrucciones cubren la configuraci√≥n de Kafka y la ejecuci√≥n de la aplicaci√≥n.

### 1. Configuraci√≥n de Kafka (en la VM)

```bash
# Instalar kafka-python
pip install kafka-python

# Descargar y configurar Kafka
wget https://downloads.apache.org/kafka/3.6.2/kafka_2.13-3.6.2.tgz
tar -xzf kafka_2.13-3.6.2.tgz
sudo mv kafka_2.13-3.6.2 /opt/Kafka

# Iniciar ZooKeeper
sudo /opt/Kafka/bin/zookeeper-server-start.sh /opt/Kafka/config/zookeeper.properties &

# Iniciar Kafka
sudo /opt/Kafka/bin/kafka-server-start.sh /opt/Kafka/config/server.properties &

# Crear topic para transacciones
/opt/Kafka/bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic transacciones_ventas
```

## Comandos de Ejecuci√≥n

### 1. Generar Dataset (Ejecutar localmente)

```bash
python3 generar_dataset_ventas.py
```

### 2. Procesamiento Batch (En la VM)

```bash
spark-submit procesamiento_batch.py
```

### 3. Streaming en Tiempo Real (En la VM)

#### Terminal 1 - Productor Kafka:
```bash
python3 kafka_productor_ventas.py
```

#### Terminal 2 - Consumidor Spark Streaming:
```bash
spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3 spark_streaming_consumidor.py
```

## Estructura del Proyecto

```
spark-ecommerce-analytics/
‚îú‚îÄ‚îÄ datos/
‚îÇ   ‚îî‚îÄ‚îÄ transacciones_ventas.csv          # Dataset generado
‚îú‚îÄ‚îÄ batch/
‚îÇ   ‚îî‚îÄ‚îÄ procesamiento_batch.py            # Script de procesamiento batch
‚îú‚îÄ‚îÄ streaming/
‚îÇ   ‚îú‚îÄ‚îÄ kafka_productor_ventas.py         # Productor de Kafka
‚îÇ   ‚îî‚îÄ‚îÄ spark_streaming_consumidor.py    # Consumidor Spark Streaming
‚îú‚îÄ‚îÄ resultados/                            # Resultados del procesamiento batch
‚îÇ   ‚îú‚îÄ‚îÄ transacciones_limpias.parquet
‚îÇ   ‚îú‚îÄ‚îÄ transacciones_con_fecha.parquet
‚îÇ   ‚îî‚îÄ‚îÄ ventas_por_categoria.parquet
‚îú‚îÄ‚îÄ README.md                             # Este archivo
‚îú‚îÄ‚îÄ requirements.txt                       # Dependencias Python
‚îî‚îÄ‚îÄ INSTRUCCIONES_EJECUCION.md           # Gu√≠a detallada de ejecuci√≥n
```

## An√°lisis Realizados

### Procesamiento Batch

1. **Limpieza de Datos**
   - Eliminaci√≥n de valores nulos
   - Eliminaci√≥n de duplicados
   - Validaci√≥n de datos

2. **An√°lisis Exploratorio**
   - Estad√≠sticas descriptivas
   - Ventas por categor√≠a
   - Ventas por m√©todo de pago
   - Top 10 productos m√°s vendidos
   - An√°lisis temporal (ventas por d√≠a/hora)
   - Distribuci√≥n de estados de transacci√≥n

### Procesamiento en Tiempo Real

1. **M√©tricas por Ventana de Tiempo (1 minuto)**
   - Total de ventas
   - N√∫mero de transacciones
   - Promedio de venta

2. **An√°lisis por Categor√≠a**
   - Ventas por categor√≠a en tiempo real
   - Ranking de categor√≠as m√°s vendidas

3. **An√°lisis de M√©todos de Pago**
   - M√©todos m√°s utilizados
   - Volumen de transacciones por m√©todo

4. **Estados de Transacci√≥n**
   - Distribuci√≥n de estados en tiempo real
   - Monitoreo de transacciones completadas vs pendientes

## Capturas de Pantalla del Proceso

**Nota**: Las capturas de pantalla est√°n organizadas por carpeta `imagenes/` e integradas en las secciones de instalaci√≥n y ejecuci√≥n arriba.

#### Actualizaci√≥n del Sistema
![Actualizaci√≥n del Sistema](imagenes/Toma%20captura%20de%20la%20actualizaci%C3%B3n%20del%20sistema.png)

#### Versiones de Python y pip
![Versiones de Python y pip](imagenes/Captura%20las%20versiones%20de%20Python%20y%20pip.png)

#### Clonaci√≥n del Repositorio
![Clonaci√≥n del Repositorio](imagenes/Toma%20captura%20de%20la%20clonaci%C3%B3n%20exitosa%20del%20repositorio.png)

### 2. Instalaci√≥n de Dependencias

#### Instalaci√≥n de Dependencias Python
![Instalaci√≥n de Dependencias](imagenes/Captura%20la%20instalaci%C3%B3n%20exitosa%20de%20las%20dependencias%20Python.png)

#### Verificaci√≥n de Librer√≠as
![Verificaci√≥n de Librer√≠as](imagenes/Captura%20la%20verificaci%C3%B3n%20de%20las%20librer%C3%ADas%20instaladas.png)

### 3. Configuraci√≥n de Servicios

#### Servicios Hadoop (jps)
![Servicios Hadoop](imagenes/Captura%20la%20salida%20de%20jps%20mostrando%20los%20servicios%20de%20Hadoop.png)

#### Versi√≥n de Spark
![Versi√≥n de Spark](imagenes/Captura%20la%20versi%C3%B3n%20de%20Spark%20instalada.png)

#### Dataset CSV Verificado
![Dataset CSV](imagenes/Captura%20la%20verificaci%C3%B3n%20del%20dataset%20CSV.png)

### 4. Configuraci√≥n de Kafka

#### Descarga y Configuraci√≥n de Kafka
![Descarga de Kafka](imagenes/Captura%20la%20descarga%20%2C%20descompresi%C3%B3n%20y%20configuraci%C3%B3n%20de%20Kafka..png)

#### Inicio de Kafka
![Inicio de Kafka](imagenes/Captura%20el%20inicio%20de%20Kafka.png)

#### Creaci√≥n del Topic
![Creaci√≥n del Topic](imagenes/Captura%20la%20creaci%C3%B3n%20exitosa%20del%20topic.png)

### 5. Procesamiento Batch

#### Ejecuci√≥n del Procesamiento Batch
![Ejecuci√≥n Batch a](imagenes/Captura%20la%20ejecuci%C3%B3n%20del%20procesamiento%20batch%20y%20los%20resultados%20del%20an%C3%A1lisis%20exploratorio%20a.png)

![Ejecuci√≥n Batch b](imagenes/Captura%20la%20ejecuci%C3%B3n%20del%20procesamiento%20batch%20y%20los%20resultados%20del%20an%C3%A1lisis%20exploratorio%20b.png)

![Ejecuci√≥n Batch c](imagenes/Captura%20la%20ejecuci%C3%B3n%20del%20procesamiento%20batch%20y%20los%20resultados%20del%20an%C3%A1lisis%20exploratorio%20c.png)

![Ejecuci√≥n Batch d](imagenes/Captura%20la%20ejecuci%C3%B3n%20del%20procesamiento%20batch%20y%20los%20resultados%20del%20an%C3%A1lisis%20exploratorio%20d.png)

#### Archivos Parquet Generados
![Archivos Parquet](imagenes/Captura%20los%20archivos%20Parquet%20generado.png)

### 6. Spark Streaming en Tiempo Real

#### Productor Kafka
![Productor Kafka](imagenes/Captura%20el%20productor%20enviando%20transacciones.png)

#### Consumidor Spark Streaming
![Consumidor Streaming](imagenes/Captura%20el%20consumidor%20procesando%20datos%20en%20tiempo%20real.png)

#### Interfaz Web de Spark
![Spark UI Jobs](imagenes/Captura%20la%20interfaz%20web%20de%20Spark%20mostrando%20jobs..png)

![Spark UI Streaming](imagenes/Captura%20la%20interfaz%20web%20de%20Spark%20mostrando%20StreamingQuery.png)

### 7. Estructura del Proyecto

#### Estructura de Directorios
![Estructura de Directorios](imagenes/Captura%20la%20estructura%20de%20directorios%20creada.png)

## Monitoreo y Visualizaci√≥n

### Interfaz Web de Spark
Acceder a: `http://192.168.0.x:4040` para ver:
- Jobs ejecut√°ndose
- Stages del procesamiento
- M√©tricas de rendimiento
- Historial de aplicaciones

### Salida en Consola
Los resultados se muestran en tiempo real en la consola con:
- Ventanas de tiempo con m√©tricas agregadas
- Rankings de categor√≠as y productos
- Estad√≠sticas de m√©todos de pago

## Resultados Esperados

### Procesamiento Batch
- Dataset limpio de ~100,000 transacciones
- Estad√≠sticas por categor√≠a (Electr√≥nica lidera en ventas)
- Distribuci√≥n temporal de ventas
- Top productos m√°s vendidos

### Streaming en Tiempo Real
- Procesamiento continuo de transacciones
- Actualizaci√≥n de m√©tricas cada minuto
- Monitoreo de patrones de venta en tiempo real
- Detecci√≥n de tendencias por categor√≠a

## Soluci√≥n de Problemas

### Error de Conexi√≥n a Kafka
```bash
# Verificar que Kafka est√© ejecut√°ndose
sudo /opt/Kafka/bin/kafka-topics.sh --list --bootstrap-server localhost:9092
```

### Error de Memoria en Spark
```bash
# Ajustar memoria en spark-submit
spark-submit --driver-memory 2g --executor-memory 2g procesamiento_batch.py
```

### Puerto 4040 Ocupado
```bash
# Cambiar puerto de la interfaz web
spark-submit --conf spark.ui.port=4041 procesamiento_batch.py
```

## Estado del Proyecto

### ‚úÖ Completado

- [x] **Configuraci√≥n del entorno**: Actualizaci√≥n del sistema, instalaci√≥n de dependencias
- [x] **Dataset generado**: 100,000 transacciones sint√©ticas de e-commerce
- [x] **Kafka configurado**: ZooKeeper y Kafka funcionando correctamente
- [x] **Topic creado**: `transacciones_ventas` listo para streaming
- [x] **Spark Streaming**: Consumidor funcionando y procesando datos en tiempo real
- [x] **Documentaci√≥n**: README completo con capturas de pantalla del proceso

### üîÑ En Progreso

- [ ] **Productor Kafka**: Ejecut√°ndose para enviar transacciones simuladas
- [ ] **Procesamiento Batch**: An√°lisis exploratorio de datos completado
- [ ] **Monitoreo**: Interfaz web de Spark disponible en puerto 4040

### üìä Resultados Obtenidos

- **Dataset**: 100,000 transacciones procesadas exitosamente
- **Kafka**: Streaming en tiempo real funcionando
- **Spark**: An√°lisis batch y streaming operativos
- **Capturas**: Documentaci√≥n completa del proceso de implementaci√≥n

## Autor

**appsgalvis** - Estudiante de Ingenier√≠a de Sistemas UNAD
Curso: Big Data (202016911)
Tarea 3: Procesamiento de Datos con Apache Spark

## Repositorio GitHub

üîó **URL del Repositorio**: [https://github.com/appsgalvis/spark-ecommerce-analytics](https://github.com/appsgalvis/spark-ecommerce-analytics)

## Licencia

Este proyecto es parte de una actividad acad√©mica de la Universidad Nacional Abierta y a Distancia (UNAD).